{% extends "base.html" %}

{% load static %}
{% block content %}
  <section class="section container">
    <div class="section-header">
      <h1>{{ challenge.name }} Challenge - {{ task.name }} {% if challenge.locked %}
        [Closed]{% endif %}</h1>
    </div>
    <div class="section-content">
      <h2><strong>Goal</strong></h2>
      <p>Submit automated predictions of lesion segmentation boundaries within dermoscopic images.
      </p>
      <img src="{% static 'img/landing/2018/task1.png' %}">
      <h2><strong>Data</strong></h2>
      <h3>Input Data</h3>
      <p>The input data are&nbsp;dermoscopic lesion images in JPEG format.</p>
      <p>All lesion images are named using the scheme&nbsp;<code>ISIC_&lt;image_id&gt;.jpg</code>,
        where <code>&lt;image_id&gt;</code>&nbsp;is a 7-digit unique identifier. EXIF tags in the
        images have been removed; any remaining EXIF tags should not be relied upon to provide
        accurate metadata.</p>
      <p>The lesion images were acquired with a variety of <a
          href="https://dermoscopedia.org/Principles_of_dermoscopy" target="_blank" rel="noopener">dermatoscope
        types</a>, from all anatomic sites (excluding mucosa and nails), from a historical sample of
        patients presented for skin cancer screening, from several different institutions. Every
        lesion image contains exactly one primary lesion; other fiducial markers, smaller secondary
        lesions, or other pigmented regions may be neglected.</p>
      <p>The distribution of disease states represent a modified “real world” setting whereby there
        are more benign lesions than malignant lesions, but an over-representation of
        malignancies.</p>
      <h3>Response Data</h3>
      <p>The response data are binary mask images in PNG format,&nbsp;indicating the location of the
        primary skin lesion within each input lesion image.</p>
      <p>Mask images are named using the
        scheme&nbsp;<code>ISIC_&lt;image_id&gt;_segmentation.png</code>,&nbsp;where <code>&lt;image_id&gt;</code>
        matches the corresponding lesion image for the mask.</p>
      <p>Mask images must have the exact same dimensions as their corresponding lesion image. Mask
        images are encoded as single-channel (grayscale) 8-bit PNGs (to provide lossless
        compression), where each pixel is either:</p>
      <ul>
        <li><code>0</code>: representing the background of the image, or areas outside the primary
          lesion
        </li>
        <li><code>255</code>: representing the foreground of the image, or areas inside the primary
          lesion
        </li>
      </ul>
      <p>As the primary skin lesion is a single contiguous region, mask images should also contain
        only a single contiguous foreground region, without any disconnected components or holes.
        The foreground region may be of any size (including the entire image) and may abut the
        borders of the image.</p>
      <h4>Ground Truth Provenance</h4>
      <p>Mask image ground truth (provided for training and used internally for scoring validation
        and test phases) data were generated using several techniques, but all data were&nbsp;reviewed
        and curated by practicing dermatologists with expertise in dermoscopy.</p>
      <p>Ground truth segmentations were generated by either:</p>
      <ul>
        <li>fully-automated algorithm, reviewed and accepted by a human expert</li>
        <li>a semi-automated flood-fill algorithm, with parameters chosen by a human expert</li>
        <li>manual polygon tracing by a human expert</li>
      </ul>
      <h2><strong>Evaluation</strong></h2>
      <h3>Goal Metric</h3>
      <p>Predicted responses are scored using a threshold Jaccard index metric.</p>
      <p>To compute this metric:</p>
      <ul>
        <li>For each image, a pixel-wise comparison of each predicted segmentation with the
          corresponding ground truth segmentation is made using the <a
              href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a>.
        </li>
        <li>The final score for each image is computed as a threshold of the Jaccard according to
          the following:
          <ul>
            <li>score = 0, if the Jaccard index is&nbsp;less than 0.65</li>
            <li>score = the Jaccard index value, otherwise</li>
          </ul>
        </li>
        <li>The mean of all per-image scores is taken as the final metric value for the entire
          dataset
        </li>
      </ul>
      <h4>Rationale</h4>
      <p>The choice of threshold Jaccard index metric is based on <a
          href="https://arxiv.org/pdf/1710.05006.pdf" target="_blank" rel="noopener">a previously
        published analysis</a> which demonstrated using the Jaccard directly as a measure of
        performance does not accurately reflect the number of images in which automated segmentation
        fails, or falls outside <a href="https://arxiv.org/abs/1610.04662">expert interobserver
          variability</a> (i.e. the raw Jaccard is overly optimistic). The number of images in which
        automated segmentation fails is a direct measure of the amount of labor required to correct
        an algorithm.</p>
      <p>In order to determine the threshold, the lowest Jaccard agreement between 3 independent
        expert annotators was measured on a subset of 100 images. This empirically measured value
        (~0.74) is the basis for the 0.65 value threshold (with additional error tolerance), which
        indicates segmentation failure on an image.</p>
      <h3>Other Metrics</h3>
      <p>Participants will be ranked and awards granted based only on the&nbsp;Threshold Jaccard
        index metric. However, for scientific completeness, predicted responses will also have the
        following metrics computed on a pixel-wise basis (comparing prediction vs. ground truth) for
        each image:</p>
      <ul>
        <li><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"
               target="_blank" rel="noopener">sensitivity</a></li>
        <li><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"
               target="_blank" rel="noopener">specificity</a></li>
        <li><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification"
               target="_blank" rel="noopener">accuracy</a></li>
        <li><a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="noopener">raw
          Jaccard index</a></li>
        <li><a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"
               target="_blank" rel="noopener">Dice coefficient</a></li>
      </ul>
      <h2><strong>Submission Instructions</strong></h2>
      <p>To participate in this task:</p>
      <ol>
        <li>Train
          <ol>
            <li>Download the training input data and training ground truth response data.</li>
            <li>Develop an algorithm for generating lesion segmentations in general.</li>
          </ol>
        </li>
        <li>Validate (optional)
          <ol>
            <li>Download the validation input data.</li>
            <li>Run your algorithm on the validation Input data to produce validation predicted
              responses.
            </li>
            <li>Submit these validation predicted responses to receive an immediate score. This will
              provide feedback that your predicted responses have the correct data format and have
              reasonable performance. You may make unlimited submissions.
            </li>
          </ol>
        </li>
        <li>Test
          <ol>
            <li>Download the test input data.</li>
            <li>Run your algorithm on the test input data to produce test predicted responses.</li>
            <li>Submit these test predicted responses. You may submit a maximum of 3 separate
              approaches/algorithms to be evaluated independently. You may make unlimited
              submissions, but only the most recent submission for each approach will be used for
              official judging. Use the “brief description of your algorithm’s approach” field on
              the submission form to distinguish different approaches. Previously submitted
              approaches are available in the dropdown menu.
            </li>
            <li>Submit a manuscript describing your algorithm’s approach.</li>
          </ol>
        </li>
      </ol>
    </div>
  </section>
{% endblock %}
